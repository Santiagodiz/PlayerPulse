{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\oscac\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oscac\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (2024.2.2)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium\n",
    "%pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credenciales de Twitter\n",
    "USERNAME = 'ScrapeoUPM'\n",
    "PASSWORD = 'scrapeafutbol'\n",
    "EMAIL = 'oscar.marin@alumnos.upm.es'\n",
    "\n",
    "SCROLL_PAUSE_TIME = 3  # Tiempo de espera para que carguen nuevos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(driver, username, email, password):\n",
    "    \n",
    "    # Navegar a la página de inicio de sesión de Twitter\n",
    "    driver.get('https://twitter.com/login')\n",
    "    time.sleep(7)  # Esperar a que cargue la página\n",
    "\n",
    "    # Buscar el campo del nombre de usuario e ingresar el nombre de usuario o correo\n",
    "    username_input = driver.find_element(By.NAME, 'text')\n",
    "    username_input.send_keys(username)\n",
    "    username_input.send_keys(Keys.RETURN) #Presionar tecla enter\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        # Buscar el campo de confirmación de correo\n",
    "        email_input = driver.find_element(By.NAME, 'text')\n",
    "        email_input.send_keys(email)\n",
    "        email_input.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Buscar el campo de contraseña e ingresar la contraseña\n",
    "    password_input = driver.find_element(By.NAME, 'password')\n",
    "    password_input.send_keys(password)\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "    time.sleep(5)\n",
    "\n",
    "def scrap_n_tweets(driver, n_tweets: int, word_search: str, date_range: tuple):\n",
    "\n",
    "    word_search = word_search.replace(' ', '%20')\n",
    "    date_range = f'%20until%3A{date_range[1]}%20since%3A{date_range[0]}'\n",
    "\n",
    "    # Navegar a la página de búsqueda de Twitter\n",
    "    driver.get(f'https://twitter.com/search?q={word_search}{date_range}')\n",
    "    time.sleep(5)  # Esperar a que cargue la página\n",
    "\n",
    "    n = 0\n",
    "    tweets = []\n",
    "    while n < n_tweets:\n",
    "        # Extraer los tweets visibles en este momento\n",
    "        new_tweets = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"tweet\"]')\n",
    "        \n",
    "        # Imprimir el texto de los tweets extraídos\n",
    "        for tweet in new_tweets:\n",
    "            tweet_text = tweet.find_element(By.CSS_SELECTOR, 'div[data-testid=\"tweetText\"]')\n",
    "            tweet_date = tweet.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')\n",
    "            tweet_lang  = tweet.find_element(By.CSS_SELECTOR, 'div[data-testid=\"tweetText\"]').get_attribute('lang')\n",
    "            tweets.append([word_search, tweet_text.text, tweet_date, tweet_lang])\n",
    "            n += 1\n",
    "            if n == n_tweets:\n",
    "                break\n",
    "\n",
    "        # Hacer scroll hasta el final de la página\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def scrap_all_tweets(driver, word_search: str, date_range: tuple):\n",
    "    \n",
    "    word_search = word_search.replace(' ', '%20')\n",
    "    date_range = f'%20until%3A{date_range[1]}%20since%3A{date_range[0]}'\n",
    "\n",
    "    # Navegar a la página de búsqueda de Twitter\n",
    "    driver.get(f'https://twitter.com/search?q={word_search}{date_range}')\n",
    "    time.sleep(5)  # Esperar a que cargue la página\n",
    "  \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    tweets = []\n",
    "    while True:\n",
    "        # Extraer los tweets visibles en este momento\n",
    "        new_tweets = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"tweet\"]')\n",
    "        \n",
    "        # Imprimir el texto de los tweets extraídos\n",
    "        for tweet in new_tweets:\n",
    "            tweet_text = tweet.find_element(By.CSS_SELECTOR, 'div[data-testid=\"tweetText\"]')\n",
    "            tweet_date = tweet.find_element(By.CSS_SELECTOR, 'time').get_attribute('datetime')\n",
    "            tweet_lang  = tweet.find_element(By.CSS_SELECTOR, 'div[data-testid=\"tweetText\"]').get_attribute('lang')\n",
    "            tweets.append([word_search, tweet_text.text, tweet_date, tweet_lang])\n",
    "            \n",
    "        # Hacer scroll hasta el final de la página\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2024-09-30', '2024-10-06'),\n",
       " ('2024-10-07', '2024-10-13'),\n",
       " ('2024-10-14', '2024-10-20'),\n",
       " ('2024-10-21', '2024-10-27'),\n",
       " ('2024-10-28', '2024-11-03'),\n",
       " ('2024-11-04', '2024-11-06')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weeks(since, until):\n",
    "    weeks = []\n",
    "    actual_date = datetime.strptime(since, '%Y-%m-%d')\n",
    "    final_date = datetime.strptime(until, '%Y-%m-%d')\n",
    "\n",
    "    while actual_date + timedelta(days=7) < final_date:\n",
    "        # Definir el rango de cada semana\n",
    "        week_end = actual_date + timedelta(days=6) # Día actual + 6 días = 7 días\n",
    "\n",
    "        weeks.append((actual_date.strftime('%Y-%m-%d'), week_end.strftime('%Y-%m-%d')))\n",
    "        # Avanzar a la siguiente semana\n",
    "        actual_date = week_end + timedelta(days=1) # Empieza el siguiente día para no repetir\n",
    "\n",
    "    weeks.append((actual_date.strftime('%Y-%m-%d'), until))\n",
    "    return weeks\n",
    "\n",
    "get_weeks('2024-09-30', '2024-11-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets from 2023-08-01 to 2023-08-07\n",
      "Found 112 tweets\n",
      "Scraping tweets from 2023-08-08 to 2023-08-14\n",
      "Found 158 tweets\n",
      "Scraping tweets from 2023-08-15 to 2023-08-21\n",
      "Found 163 tweets\n",
      "Scraping tweets from 2023-08-22 to 2023-08-28\n",
      "Found 31 tweets\n",
      "Scraping tweets from 2023-08-29 to 2023-09-04\n",
      "Found 28 tweets\n",
      "Scraping tweets from 2023-09-05 to 2023-09-11\n",
      "Found 30 tweets\n",
      "Scraping tweets from 2023-09-12 to 2023-09-18\n",
      "Found 11 tweets\n",
      "Scraping tweets from 2023-09-19 to 2023-09-25\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-09-26 to 2023-10-02\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-10-03 to 2023-10-09\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-10-10 to 2023-10-16\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-10-17 to 2023-10-23\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-10-24 to 2023-10-30\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-10-31 to 2023-11-06\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-11-07 to 2023-11-13\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-11-14 to 2023-11-20\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-11-21 to 2023-11-27\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-11-28 to 2023-12-04\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-12-05 to 2023-12-11\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-12-12 to 2023-12-18\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-12-19 to 2023-12-25\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2023-12-26 to 2024-01-01\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-01-02 to 2024-01-08\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-01-09 to 2024-01-15\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-01-16 to 2024-01-22\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-01-23 to 2024-01-29\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-01-30 to 2024-02-05\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-02-06 to 2024-02-12\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-02-13 to 2024-02-19\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-02-20 to 2024-02-26\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-02-27 to 2024-03-04\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-03-05 to 2024-03-11\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-03-12 to 2024-03-18\n",
      "Found 0 tweets\n",
      "Scraping tweets from 2024-03-19 to 2024-03-25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m since, until \u001b[38;5;129;01min\u001b[39;00m weeks:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping tweets from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muntil\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mscrap_all_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msince\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muntil\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tweets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tweets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     all_tweets\u001b[38;5;241m.\u001b[39mextend(tweets)\n",
      "Cell \u001b[1;32mIn[48], line 83\u001b[0m, in \u001b[0;36mscrap_all_tweets\u001b[1;34m(driver, word_search, date_range)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Hacer scroll hasta el final de la página\u001b[39;00m\n\u001b[0;32m     82\u001b[0m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCROLL_PAUSE_TIME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m new_height \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn document.body.scrollHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_height \u001b[38;5;241m==\u001b[39m last_height:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "player = 'Ferran Torres'\n",
    "since = '2023-08-01'\n",
    "until = '2024-07-15'\n",
    "n_tweets = 100\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Ejecuta Chrome en modo headless\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "login(driver, USERNAME, EMAIL, PASSWORD)\n",
    "\n",
    "weeks = get_weeks(since, until)\n",
    "\n",
    "all_tweets = []\n",
    "for since, until in weeks:\n",
    "    print(f'Scraping tweets from {since} to {until}')\n",
    "    tweets = scrap_all_tweets(driver, player, (since, until))\n",
    "    print(f'Found {len(tweets)} tweets')\n",
    "    all_tweets.extend(tweets)\n",
    "\n",
    "# Crear un archivo CSV con los tweets extraídos\n",
    "with open(f'/raw/{player.replace(' ', '_')}_tweets.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['text', 'date', 'lang'])\n",
    "    for tweet in all_tweets:\n",
    "        writer.writerow(tweet)\n",
    "\n",
    "# Cerrar el navegador\n",
    "driver.quit()\n",
    "tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
